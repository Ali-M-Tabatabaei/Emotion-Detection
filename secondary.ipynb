{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from tensorflow.keras.applications import VGG16, InceptionResNetV2\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax\n",
    "from keras.models import model_from_json \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"./archive/train\" \n",
    "test_dir = \"./archive/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/Desktop/project/Emotion Detection/venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model= tf.keras.models.Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))\n",
    "model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(5,5), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Conv2D(512,(3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(512,(3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Dense(512,activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=48\n",
    "train_datagen = ImageDataGenerator(width_shift_range = 0.1,\n",
    "                                         height_shift_range = 0.1,\n",
    "                                         horizontal_flip = True,\n",
    "                                         rescale = 1./255,\n",
    "                                         validation_split = 0.2\n",
    "                                        )\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                         validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 1432 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                    target_size = (img_size,img_size),\n",
    "                                                    batch_size = 64,\n",
    "                                                    color_mode = \"grayscale\",\n",
    "                                                    class_mode = \"categorical\",\n",
    "                                                    subset = \"training\"\n",
    "                                                   )\n",
    "validation_generator = validation_datagen.flow_from_directory( directory = test_dir,\n",
    "                                                              target_size = (img_size,img_size),\n",
    "                                                              batch_size = 64,\n",
    "                                                              color_mode = \"grayscale\",\n",
    "                                                              class_mode = \"categorical\",\n",
    "                                                              subset = \"validation\"\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = Adam(learning_rate=0.001), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/Desktop/project/Emotion Detection/venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 749ms/step - accuracy: 0.1985 - loss: 2.2807 - val_accuracy: 0.2500 - val_loss: 1.8688\n",
      "Epoch 2/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 762ms/step - accuracy: 0.2794 - loss: 1.8429 - val_accuracy: 0.2961 - val_loss: 1.9008\n",
      "Epoch 3/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 757ms/step - accuracy: 0.3714 - loss: 1.6262 - val_accuracy: 0.3282 - val_loss: 1.8828\n",
      "Epoch 4/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 712ms/step - accuracy: 0.4434 - loss: 1.4455 - val_accuracy: 0.4909 - val_loss: 1.3382\n",
      "Epoch 5/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 676ms/step - accuracy: 0.4979 - loss: 1.3246 - val_accuracy: 0.5070 - val_loss: 1.3022\n",
      "Epoch 6/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 676ms/step - accuracy: 0.5152 - loss: 1.2614 - val_accuracy: 0.5496 - val_loss: 1.2164\n",
      "Epoch 7/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 676ms/step - accuracy: 0.5418 - loss: 1.1931 - val_accuracy: 0.5719 - val_loss: 1.1515\n",
      "Epoch 8/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 677ms/step - accuracy: 0.5566 - loss: 1.1552 - val_accuracy: 0.5168 - val_loss: 1.3014\n",
      "Epoch 9/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 726ms/step - accuracy: 0.5719 - loss: 1.1361 - val_accuracy: 0.6061 - val_loss: 1.1001\n",
      "Epoch 10/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 737ms/step - accuracy: 0.5870 - loss: 1.0987 - val_accuracy: 0.5873 - val_loss: 1.1195\n",
      "Epoch 11/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 757ms/step - accuracy: 0.5768 - loss: 1.1012 - val_accuracy: 0.6159 - val_loss: 1.0364\n",
      "Epoch 12/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 777ms/step - accuracy: 0.5935 - loss: 1.0634 - val_accuracy: 0.3827 - val_loss: 1.6544\n",
      "Epoch 13/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 786ms/step - accuracy: 0.5966 - loss: 1.0668 - val_accuracy: 0.6061 - val_loss: 1.0375\n",
      "Epoch 14/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 818ms/step - accuracy: 0.6072 - loss: 1.0355 - val_accuracy: 0.6145 - val_loss: 1.0369\n",
      "Epoch 15/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 824ms/step - accuracy: 0.6101 - loss: 1.0322 - val_accuracy: 0.6131 - val_loss: 1.0297\n",
      "Epoch 16/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 841ms/step - accuracy: 0.6181 - loss: 1.0029 - val_accuracy: 0.5985 - val_loss: 1.0491\n",
      "Epoch 17/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 805ms/step - accuracy: 0.6239 - loss: 1.0016 - val_accuracy: 0.6362 - val_loss: 0.9991\n",
      "Epoch 18/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 789ms/step - accuracy: 0.6337 - loss: 0.9770 - val_accuracy: 0.6152 - val_loss: 1.0684\n",
      "Epoch 19/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 737ms/step - accuracy: 0.6248 - loss: 0.9780 - val_accuracy: 0.6236 - val_loss: 1.0010\n",
      "Epoch 20/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 710ms/step - accuracy: 0.6400 - loss: 0.9579 - val_accuracy: 0.6278 - val_loss: 1.0158\n",
      "Epoch 21/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 731ms/step - accuracy: 0.6377 - loss: 0.9612 - val_accuracy: 0.6222 - val_loss: 1.0200\n",
      "Epoch 22/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 796ms/step - accuracy: 0.6457 - loss: 0.9374 - val_accuracy: 0.6508 - val_loss: 0.9702\n",
      "Epoch 23/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 824ms/step - accuracy: 0.6499 - loss: 0.9300 - val_accuracy: 0.6404 - val_loss: 1.0026\n",
      "Epoch 24/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 798ms/step - accuracy: 0.6569 - loss: 0.9171 - val_accuracy: 0.6522 - val_loss: 0.9811\n",
      "Epoch 25/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 769ms/step - accuracy: 0.6629 - loss: 0.8976 - val_accuracy: 0.6592 - val_loss: 0.9492\n",
      "Epoch 26/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 769ms/step - accuracy: 0.6632 - loss: 0.8966 - val_accuracy: 0.6194 - val_loss: 1.0410\n",
      "Epoch 27/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 759ms/step - accuracy: 0.6679 - loss: 0.8798 - val_accuracy: 0.6536 - val_loss: 0.9448\n",
      "Epoch 28/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 716ms/step - accuracy: 0.6730 - loss: 0.8787 - val_accuracy: 0.6292 - val_loss: 1.0244\n",
      "Epoch 29/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 715ms/step - accuracy: 0.6794 - loss: 0.8615 - val_accuracy: 0.6690 - val_loss: 0.9471\n",
      "Epoch 30/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 714ms/step - accuracy: 0.6841 - loss: 0.8399 - val_accuracy: 0.6543 - val_loss: 0.9584\n",
      "Epoch 31/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 715ms/step - accuracy: 0.6753 - loss: 0.8502 - val_accuracy: 0.6487 - val_loss: 1.0211\n",
      "Epoch 32/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 716ms/step - accuracy: 0.6915 - loss: 0.8268 - val_accuracy: 0.6411 - val_loss: 0.9643\n",
      "Epoch 33/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 715ms/step - accuracy: 0.6882 - loss: 0.8269 - val_accuracy: 0.6404 - val_loss: 0.9465\n",
      "Epoch 34/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 716ms/step - accuracy: 0.6894 - loss: 0.8282 - val_accuracy: 0.6466 - val_loss: 0.9629\n",
      "Epoch 35/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 716ms/step - accuracy: 0.7011 - loss: 0.8037 - val_accuracy: 0.6599 - val_loss: 0.9616\n",
      "Epoch 36/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 715ms/step - accuracy: 0.7043 - loss: 0.7959 - val_accuracy: 0.6564 - val_loss: 0.9509\n",
      "Epoch 37/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 717ms/step - accuracy: 0.7029 - loss: 0.7824 - val_accuracy: 0.6634 - val_loss: 0.9534\n",
      "Epoch 38/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 733ms/step - accuracy: 0.7067 - loss: 0.7811 - val_accuracy: 0.6446 - val_loss: 0.9970\n",
      "Epoch 39/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 784ms/step - accuracy: 0.7048 - loss: 0.7916 - val_accuracy: 0.6585 - val_loss: 0.9956\n",
      "Epoch 40/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 789ms/step - accuracy: 0.7080 - loss: 0.7765 - val_accuracy: 0.6788 - val_loss: 0.9589\n",
      "Epoch 41/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 807ms/step - accuracy: 0.7107 - loss: 0.7652 - val_accuracy: 0.6473 - val_loss: 1.0246\n",
      "Epoch 42/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 799ms/step - accuracy: 0.7172 - loss: 0.7592 - val_accuracy: 0.6592 - val_loss: 0.9724\n",
      "Epoch 43/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 814ms/step - accuracy: 0.7224 - loss: 0.7572 - val_accuracy: 0.6327 - val_loss: 1.0228\n",
      "Epoch 44/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 794ms/step - accuracy: 0.7265 - loss: 0.7283 - val_accuracy: 0.6648 - val_loss: 0.9582\n",
      "Epoch 45/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 795ms/step - accuracy: 0.7303 - loss: 0.7153 - val_accuracy: 0.6585 - val_loss: 0.9993\n"
     ]
    }
   ],
   "source": [
    "epochs = 45\n",
    "batch_size = 64\n",
    "history = model.fit(x = train_generator,epochs = epochs,validation_data = validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json() \n",
    "with open(\"emotion_model_secondary.json\", \"w\") as json_file: \n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('emotion_model_secondary.json', 'r') \n",
    "loaded_model_json = json_file.read() \n",
    "json_file.close() \n",
    "model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "img_height = 48\n",
    "img_width = 48\n",
    "\n",
    "# Preprocess the input image\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, color_mode='grayscale', target_size=(img_height, img_width))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = img_array / 255.0  # Normalize pixel values\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "def predict_emotion_with_image(image_path):\n",
    "    # Load and display the image\n",
    "    img = load_img(image_path, color_mode='grayscale', target_size=(img_height, img_width))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Preprocess the image\n",
    "    preprocessed_img = preprocess_image(image_path)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(preprocessed_img)\n",
    "    predicted_class = tf.argmax(predictions[0]).numpy()\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "\n",
    "def class_to_emotion(predicted_emotion):\n",
    "    res_dict = {0: 'angry', \n",
    "                1: 'disgusted',\n",
    "                2: 'fearful',\n",
    "                3: 'happy',\n",
    "                4: 'neutral',\n",
    "                5: 'sad',\n",
    "                6: 'surprised'\n",
    "               }\n",
    "    \n",
    "    return res_dict[predicted_emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted emotion class: happy\n"
     ]
    }
   ],
   "source": [
    "image_path = \"./archive/test/happy/im30.png\"\n",
    "predicted_emotion = predict_emotion_with_image(image_path)\n",
    "print(\"Predicted emotion class:\", class_to_emotion(predicted_emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/ali/Desktop/project/Emotion Detection/venv/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Sequential' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     emotion_prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(cropped_img) \n\u001b[1;32m     25\u001b[0m     maxindex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(emotion_prediction)) \n\u001b[0;32m---> 26\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmaxindex\u001b[49m\u001b[43m]\u001b[49m, (x\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m5\u001b[39m, y\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m),  \n\u001b[1;32m     27\u001b[0m                 cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA) \n\u001b[1;32m     29\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmotion Detection\u001b[39m\u001b[38;5;124m'\u001b[39m, frame) \n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m): \n",
      "\u001b[0;31mTypeError\u001b[0m: 'Sequential' object is not subscriptable"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0) \n",
    "while True: \n",
    "    # Find haar cascade to draw bounding box around face \n",
    "    ret, frame = cap.read() \n",
    "    frame = cv2.resize(frame, (1280, 720)) \n",
    "    if not ret: \n",
    "        print(ret) \n",
    "    # Create a face detector \n",
    "    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') \n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "    # detect faces available on camera \n",
    "    num_faces = face_detector.detectMultiScale(gray_frame,  \n",
    "                                            scaleFactor=1.3, minNeighbors=5) \n",
    "\n",
    "    # take each face available on the camera and Preprocess it \n",
    "    for (x, y, w, h) in num_faces: \n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4) \n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w] \n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame,  \n",
    "                                                                (48, 48)), -1), 0) \n",
    "        # predict the emotions \n",
    "        emotion_prediction = model.predict(cropped_img) \n",
    "        maxindex = int(np.argmax(emotion_prediction)) \n",
    "        cv2.putText(frame, model[maxindex], (x+5, y-20),  \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA) \n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame) \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# save trained model weight in .h5 file \n",
    "model.save_weights('emotion_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
