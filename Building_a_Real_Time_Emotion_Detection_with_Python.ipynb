{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Projects-/blob/master/Building_a_Real_Time_Emotion_Detection_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZFQQORCAcS"
      },
      "source": [
        "# Building a Real Time Emotion Detection with Python\n",
        "# **Introduction:**\n",
        "\n",
        "Detecting real-time emotion of the person with a camera input is one of the advanced features in the machine learning process. The detection of emotion of a person using a camera is useful for various research and analytics purposes. The detection of emotion is made by using the machine learning concept. You can use the trained dataset to detect the emotion of the human being. For detecting the different emotions, first you need to train those different emotions, or you can use a dataset already available on the internet. In this article, we will discuss creating a Python program to detect real-time emotion of a human being using the camera '[1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAVZc4ayyDWL"
      },
      "source": [
        "# 1-Installing Dependencies "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "WpWN1sJv8eho",
        "outputId": "b09ba454-dd8b-44b1-91ea-8b165d079a43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in ./venv/lib/python3.10/site-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.17.0 in ./venv/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QWA9EJ-zwyej"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in ./venv/lib/python3.10/site-packages (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in ./venv/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: libclang>=13.0.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: h5py>=3.10.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./venv/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: optree in ./venv/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: namex in ./venv/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: rich in ./venv/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "b6X6cfXcw4uL",
        "outputId": "3a7e31d5-f028-4e75-daae-b4095ecc7eb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Le9Nov-7w7X6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in ./venv/lib/python3.10/site-packages (2.2.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in ./venv/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oFMnGbhH9sIS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in ./venv/lib/python3.10/site-packages (3.4.1)\n",
            "Requirement already satisfied: h5py in ./venv/lib/python3.10/site-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: rich in ./venv/lib/python3.10/site-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: absl-py in ./venv/lib/python3.10/site-packages (from keras) (2.1.0)\n",
            "Requirement already satisfied: namex in ./venv/lib/python3.10/site-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: ml-dtypes in ./venv/lib/python3.10/site-packages (from keras) (0.4.0)\n",
            "Requirement already satisfied: optree in ./venv/lib/python3.10/site-packages (from keras) (0.12.1)\n",
            "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.10/site-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install keras  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V5Pia2KM9x9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: adam in ./venv/lib/python3.10/site-packages (0.0.0.dev0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install adam  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vnF8AVoy93tt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement kwargs (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for kwargs\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kwargs  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dy8GBKBY99lX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cinit in ./venv/lib/python3.10/site-packages (0.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: np_utils in ./venv/lib/python3.10/site-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.0 in ./venv/lib/python3.10/site-packages (from np_utils) (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install cinit  \n",
        "%pip install np_utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnY7VuCDI-qN"
      },
      "source": [
        "# 1 **Training the Dataset**#\n",
        "\n",
        "For training purposes, I use the predefined un trained dataset CSV file as my main input for my input for training the machine. You can use the code given below for training the machine using the dataset. Before that, you need to ensure that all required files in the same repository where the program presents otherwise it will through some error. You can download the data set by clicking here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nhYP15aoCAcf"
      },
      "source": [
        "# 2 - **Import library** \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y1wsGi52CAch"
      },
      "outputs": [],
      "source": [
        "import sys, os  \n",
        "import pandas as pd  \n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xfWGnlhx_GYu",
        "outputId": "22786b22-1b51-4fbe-f146-cb4d85039262"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential  \n",
        "from keras.layers import Dense, Dropout, Activation, Flatten  \n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D  \n",
        "from keras.losses import categorical_crossentropy  \n",
        "from keras.optimizers import Adam  \n",
        "from keras.regularizers import l2  \n",
        "import np_utils  \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# pd.set_option('display.max_rows', 500)  \n",
        "# pd.set_option('display.max_columns', 500)  \n",
        "# pd.set_option('display.width', 1000)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XQFJb0YzEYO"
      },
      "source": [
        "#2 - **Load Dataset** # \n",
        "The both training and evaluation operations would be handled with Fec2013 dataset. Compressed version of the dataset takes 92 MB space whereas uncompressed version takes 295 MB space. There are 28K training and 3K testing images in the dataset. Each image was stored as 48×48 pixel. The pure dataset consists of image pixels (48×48=2304 values), emotion of each image and usage type (as train or test instance)[2]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "hncBMFUXFN6G",
        "outputId": "84c2788f-3f6b-4618-8062-356efc144f49"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-13a50f87-6a08-414d-b170-b5f790de68c7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-13a50f87-6a08-414d-b170-b5f790de68c7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving la.jfif to la.jfif\n"
          ]
        }
      ],
      "source": [
        "# this code is used to upload dataset from Pc to colab\n",
        "from google.colab import files # Please First run this cod in chrom \n",
        "def getLocalFiles():\n",
        "    _files = files.upload() # upload StudentNextSessionf.csv datase\n",
        "    if len(_files) >0: # Then run above  libray \n",
        "       for k,v in _files.items():\n",
        "         open(k,'wb').write(v)\n",
        "getLocalFiles()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qPlEL8_ADW3F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-07-20 19:40:50--  https://www.kaggle.com/deadskull7/fer2013\n",
            "Resolving www.kaggle.com (www.kaggle.com)... 35.244.233.98\n",
            "Connecting to www.kaggle.com (www.kaggle.com)|35.244.233.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /datasets/deadskull7/fer2013 [following]\n",
            "--2024-07-20 19:40:50--  https://www.kaggle.com/datasets/deadskull7/fer2013\n",
            "Reusing existing connection to www.kaggle.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘fer2013’\n",
            "\n",
            "fer2013                 [ <=>                ]   8.76K  --.-KB/s    in 0.004s  \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2024-07-20 19:40:50 (2.36 MB/s) - ‘fer2013’ saved [8974]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -N https://www.kaggle.com/deadskull7/fer2013"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rqD8iiJZzTZj"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('./fer2013.csv') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpHsrVexXuWj"
      },
      "source": [
        "#2-Data Description "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ia4AAHlC6Mhb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35887 entries, 0 to 35886\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   emotion  35887 non-null  int64 \n",
            " 1   pixels   35887 non-null  object\n",
            " 2   Usage    35887 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 841.2+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(df.info())  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KS0gKFmiHAvq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usage\n",
            "Training       28709\n",
            "PublicTest      3589\n",
            "PrivateTest     3589\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df[\"Usage\"].value_counts())  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zimrlUU8HEy3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n"
          ]
        }
      ],
      "source": [
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4QYI2IoHWpL"
      },
      "source": [
        "# 3- **Data Spliting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "p9JGZRKCCAcq"
      },
      "outputs": [],
      "source": [
        "X_train,train_y,X_test,test_y=[],[],[],[]  \n",
        "\n",
        "for index, row in df.iterrows():  \n",
        "    val=row['pixels'].split(\" \")  \n",
        "    try:  \n",
        "        if 'Training' in row['Usage']:  \n",
        "           X_train.append(np.array(val,'float32'))  \n",
        "           train_y.append(row['emotion'])  \n",
        "        elif 'PublicTest' in row['Usage']:  \n",
        "           X_test.append(np.array(val,'float32'))  \n",
        "           test_y.append(row['emotion'])  \n",
        "    except:  \n",
        "        print(f\"error occured at index :{index} and row:{row}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6333A5woSWX_"
      },
      "outputs": [],
      "source": [
        "num_features = 64  \n",
        "num_labels = 7  \n",
        "batch_size = 64  \n",
        "epochs = 30  \n",
        "width, height = 48, 48"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YsFBKUfkSYeB"
      },
      "outputs": [],
      "source": [
        "X_train = np.array(X_train,'float32')  \n",
        "train_y = np.array(train_y,'float32')  \n",
        "X_test = np.array(X_test,'float32')  \n",
        "test_y = np.array(test_y,'float32')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QcTmmZATSh5k"
      },
      "outputs": [],
      "source": [
        "train_y=to_categorical(train_y, num_classes=num_labels)  \n",
        "test_y=to_categorical(test_y, num_classes=num_labels) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehTTQE9YJF1Q"
      },
      "source": [
        "#4- **Normalizing data between 0 and 1** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TvqFi8-XCAcy",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "X_train -= np.mean(X_train, axis=0)  \n",
        "X_train /= np.std(X_train, axis=0)  \n",
        "X_test -= np.mean(X_test, axis=0)  \n",
        "X_test /= np.std(X_test, axis=0)  \n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)  \n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izbM6eEUKAV-"
      },
      "source": [
        "# 5- **Designing the CNN**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrgUo8fTbXz"
      },
      "source": [
        "##5.1- **1st convolution layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zt-vctO1Kb3o"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ali/Desktop/project/Emotion Detection/venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()  \n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))  \n",
        "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))  \n",
        "# model.add(BatchNormalization())  \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
        "model.add(Dropout(0.5))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x8QNZN3CAdF"
      },
      "source": [
        "## 5.2 - **2nd Convolution Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zNBdXxiHLfLW"
      },
      "outputs": [],
      "source": [
        "model.add(Conv2D(64, (3, 3), activation='relu'))  \n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))  \n",
        "# model.add(BatchNormalization())  \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
        "model.add(Dropout(0.5)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLfUeKr2UNe7"
      },
      "source": [
        "## 5.3- **3rd Convolution Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pUJ16arHaKMT"
      },
      "outputs": [],
      "source": [
        "model.add(Conv2D(128, (3, 3), activation='relu'))  \n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))  \n",
        "# model.add(BatchNormalization())  \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
        "model.add(Flatten())  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7EabEFjUl6t"
      },
      "source": [
        "##5.4-  **Fully connected neural network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "X_XHGn_oUvMG"
      },
      "outputs": [],
      "source": [
        "model.add(Dense(1024, activation='relu'))  \n",
        "model.add(Dropout(0.2))  \n",
        "model.add(Dense(1024, activation='relu'))  \n",
        "model.add(Dropout(0.2))  \n",
        "model.add(Dense(num_labels, activation='softmax'))  \n",
        "# model.summary()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-DCjB-Io3j7"
      },
      "source": [
        "#**6-Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V_1M1FnMN2O"
      },
      "source": [
        "##**6.1 Method1 Generator [2]**\n",
        "We can train the network. To complete the training in less time, I prefer to implement learning with randomly selected trainset instances. That is the reason why train and fit generator used. Also, loss function would be cross entropy because the task is multi class classification [2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Fc_KJ0lWlm8W"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "import keras\n",
        "import keras.utils\n",
        "from keras import utils as np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "u1KyuvVXMRbr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ali/Desktop/project/Emotion Detection/venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 192ms/step - accuracy: 0.2145 - loss: 1.8497\n",
            "Epoch 2/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 231ms/step - accuracy: 0.2548 - loss: 1.8150\n",
            "Epoch 3/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 260ms/step - accuracy: 0.2600 - loss: 1.7902\n",
            "Epoch 4/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 273ms/step - accuracy: 0.2475 - loss: 1.7667\n",
            "Epoch 5/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 294ms/step - accuracy: 0.2873 - loss: 1.7393\n",
            "Epoch 6/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 256ms/step - accuracy: 0.3228 - loss: 1.6918\n",
            "Epoch 7/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 256ms/step - accuracy: 0.3445 - loss: 1.6415\n",
            "Epoch 8/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260us/step - accuracy: 0.4062 - loss: 1.6357 \n",
            "Epoch 9/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-20 19:52:31.868017: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "\t [[{{node IteratorGetNext}}]]\n",
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 245ms/step - accuracy: 0.3467 - loss: 1.6109\n",
            "Epoch 10/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 247ms/step - accuracy: 0.3830 - loss: 1.5620\n",
            "Epoch 11/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 280ms/step - accuracy: 0.4011 - loss: 1.5334\n",
            "Epoch 12/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 285ms/step - accuracy: 0.3972 - loss: 1.5316\n",
            "Epoch 13/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 287ms/step - accuracy: 0.4166 - loss: 1.4985\n",
            "Epoch 14/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 267ms/step - accuracy: 0.4472 - loss: 1.4394\n",
            "Epoch 15/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 270ms/step - accuracy: 0.4397 - loss: 1.4590\n",
            "Epoch 16/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148us/step - accuracy: 0.4375 - loss: 1.3165 \n",
            "Epoch 17/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-20 19:54:32.638038: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "\t [[{{node IteratorGetNext}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 270ms/step - accuracy: 0.4322 - loss: 1.4410\n",
            "Epoch 18/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 267ms/step - accuracy: 0.4420 - loss: 1.4386\n",
            "Epoch 19/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 268ms/step - accuracy: 0.4561 - loss: 1.4134\n",
            "Epoch 20/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 269ms/step - accuracy: 0.4533 - loss: 1.3936\n",
            "Epoch 21/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 265ms/step - accuracy: 0.4568 - loss: 1.4056\n",
            "Epoch 22/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 268ms/step - accuracy: 0.4933 - loss: 1.3562\n",
            "Epoch 23/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 269ms/step - accuracy: 0.4785 - loss: 1.3541\n",
            "Epoch 24/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156us/step - accuracy: 0.5469 - loss: 1.2454 \n",
            "Epoch 25/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 264ms/step - accuracy: 0.4765 - loss: 1.3513\n",
            "Epoch 26/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 268ms/step - accuracy: 0.4883 - loss: 1.3410\n",
            "Epoch 27/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 264ms/step - accuracy: 0.4848 - loss: 1.3503\n",
            "Epoch 28/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 266ms/step - accuracy: 0.4913 - loss: 1.3249\n",
            "Epoch 29/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 268ms/step - accuracy: 0.4801 - loss: 1.3419\n",
            "Epoch 30/30\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 268ms/step - accuracy: 0.5161 - loss: 1.2903\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cb1dc77a560>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "gen = ImageDataGenerator()\n",
        "train_generator = gen.flow(X_train, train_y, batch_size=batch_size)\n",
        "model.compile(loss='categorical_crossentropy'\n",
        ", optimizer=keras.optimizers.Adam()\n",
        ", metrics=['accuracy']\n",
        ")\n",
        "model.fit(train_generator, steps_per_epoch=batch_size, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOCxPs8sU5aP"
      },
      "source": [
        "##6.2-**Method 2 Compliling the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oi-DFb_OVBr4"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=categorical_crossentropy,  \n",
        "              optimizer=Adam(),  \n",
        "              metrics=['accuracy'])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "02q29lEXVRjz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 269ms/step - accuracy: 0.4990 - loss: 1.2975 - val_accuracy: 0.5088 - val_loss: 1.2579\n",
            "Epoch 2/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 280ms/step - accuracy: 0.5169 - loss: 1.2604 - val_accuracy: 0.5358 - val_loss: 1.2122\n",
            "Epoch 3/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 277ms/step - accuracy: 0.5266 - loss: 1.2307 - val_accuracy: 0.5244 - val_loss: 1.2144\n",
            "Epoch 4/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 276ms/step - accuracy: 0.5366 - loss: 1.2079 - val_accuracy: 0.5400 - val_loss: 1.1976\n",
            "Epoch 5/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 275ms/step - accuracy: 0.5506 - loss: 1.1728 - val_accuracy: 0.5450 - val_loss: 1.1875\n",
            "Epoch 6/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 275ms/step - accuracy: 0.5622 - loss: 1.1472 - val_accuracy: 0.5355 - val_loss: 1.2105\n",
            "Epoch 7/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 246ms/step - accuracy: 0.5643 - loss: 1.1377 - val_accuracy: 0.5506 - val_loss: 1.1685\n",
            "Epoch 8/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 227ms/step - accuracy: 0.5761 - loss: 1.1134 - val_accuracy: 0.5525 - val_loss: 1.1647\n",
            "Epoch 9/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 229ms/step - accuracy: 0.5817 - loss: 1.0959 - val_accuracy: 0.5520 - val_loss: 1.1619\n",
            "Epoch 10/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 227ms/step - accuracy: 0.5838 - loss: 1.0793 - val_accuracy: 0.5748 - val_loss: 1.1278\n",
            "Epoch 11/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 228ms/step - accuracy: 0.5969 - loss: 1.0552 - val_accuracy: 0.5676 - val_loss: 1.1474\n",
            "Epoch 12/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 227ms/step - accuracy: 0.5974 - loss: 1.0502 - val_accuracy: 0.5651 - val_loss: 1.1541\n",
            "Epoch 13/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 230ms/step - accuracy: 0.6026 - loss: 1.0339 - val_accuracy: 0.5545 - val_loss: 1.1751\n",
            "Epoch 14/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 227ms/step - accuracy: 0.6100 - loss: 1.0146 - val_accuracy: 0.5681 - val_loss: 1.1692\n",
            "Epoch 15/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 228ms/step - accuracy: 0.6227 - loss: 0.9895 - val_accuracy: 0.5665 - val_loss: 1.1600\n",
            "Epoch 16/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 227ms/step - accuracy: 0.6236 - loss: 0.9857 - val_accuracy: 0.5667 - val_loss: 1.1701\n",
            "Epoch 17/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 227ms/step - accuracy: 0.6340 - loss: 0.9590 - val_accuracy: 0.5784 - val_loss: 1.1465\n",
            "Epoch 18/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 228ms/step - accuracy: 0.6353 - loss: 0.9499 - val_accuracy: 0.5687 - val_loss: 1.1603\n",
            "Epoch 19/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 229ms/step - accuracy: 0.6486 - loss: 0.9309 - val_accuracy: 0.5606 - val_loss: 1.1918\n",
            "Epoch 20/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 227ms/step - accuracy: 0.6534 - loss: 0.9185 - val_accuracy: 0.5670 - val_loss: 1.1898\n",
            "Epoch 21/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 227ms/step - accuracy: 0.6507 - loss: 0.9084 - val_accuracy: 0.5690 - val_loss: 1.2128\n",
            "Epoch 22/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 251ms/step - accuracy: 0.6584 - loss: 0.9050 - val_accuracy: 0.5812 - val_loss: 1.1683\n",
            "Epoch 23/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 275ms/step - accuracy: 0.6685 - loss: 0.8702 - val_accuracy: 0.5687 - val_loss: 1.2193\n",
            "Epoch 24/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 272ms/step - accuracy: 0.6708 - loss: 0.8585 - val_accuracy: 0.5687 - val_loss: 1.2088\n",
            "Epoch 25/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 274ms/step - accuracy: 0.6818 - loss: 0.8342 - val_accuracy: 0.5659 - val_loss: 1.2306\n",
            "Epoch 26/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 274ms/step - accuracy: 0.6850 - loss: 0.8230 - val_accuracy: 0.5818 - val_loss: 1.2507\n",
            "Epoch 27/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 256ms/step - accuracy: 0.6910 - loss: 0.8170 - val_accuracy: 0.5762 - val_loss: 1.2200\n",
            "Epoch 28/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 273ms/step - accuracy: 0.6906 - loss: 0.8135 - val_accuracy: 0.5717 - val_loss: 1.2078\n",
            "Epoch 29/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 279ms/step - accuracy: 0.6938 - loss: 0.8064 - val_accuracy: 0.5765 - val_loss: 1.2222\n",
            "Epoch 30/30\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 280ms/step - accuracy: 0.6988 - loss: 0.7924 - val_accuracy: 0.5656 - val_loss: 1.2819\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cb1dc779600>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, train_y,  \n",
        "          batch_size=batch_size,  \n",
        "          epochs=epochs,  \n",
        "          verbose=1,  \n",
        "          validation_data=(X_test, test_y),  \n",
        "          shuffle=True)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKCwzVMnVaOm"
      },
      "source": [
        "#7-**Saving the  model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "s-PUeUv-VglW"
      },
      "outputs": [],
      "source": [
        "fer_json = model.to_json()  \n",
        "with open(\"fer.json\", \"w\") as json_file:  \n",
        "    json_file.write(fer_json)  \n",
        "model.save_weights(\"fer.weights.h5\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtpS-joPfJqi"
      },
      "source": [
        "#8-**Evaluate model [2]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S05tU0VfhVm"
      },
      "outputs": [],
      "source": [
        "train_score = model.evaluate(X_train, train_y, verbose=0)\n",
        "print('Train loss:', train_score[0])\n",
        "print('Train accuracy:', 100*train_score[1])\n",
        "test_score = model.evaluate(X_test, test_y, verbose=0)\n",
        "print('Test loss:', test_score[0])\n",
        "print('Test accuracy:', 100*test_score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UC9kS_Rv_hA"
      },
      "source": [
        "# **9-Confusion Matrix**[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmsbOnwbwJTN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        " \n",
        "pred_list = []; actual_list = []\n",
        " \n",
        "for i in predictions:\n",
        "  pred_list.append(np.argmax(i))\n",
        " \n",
        "for i in Y_test:\n",
        "  actual_list.append(np.argmax(i))\n",
        " \n",
        "confusion_matrix(actual_list, pred_list)\n",
        "Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8WWRWP0wh2Z"
      },
      "source": [
        "#**10-Testing**[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "sjDLBzNZw7Tf",
        "outputId": "d861f283-6eaf-45de-e6d0-a3112c679f73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from matplotlib import pyplot as plt\n",
        "img = image.load_img(\"la.jfif\", grayscale=True, target_size=(48, 48))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis = 0)\n",
        "x /= 255\n",
        "custom = model.predict(x)\n",
        "emotion_analysis(custom[0])\n",
        "x = np.array(x, 'float32')\n",
        "x = x.reshape([48, 48]);\n",
        "plt.gray()\n",
        "plt.imshow(x)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5Te0y5lxDan"
      },
      "source": [
        "Emotions stored as numerical as labeled from 0 to 6. Keras would produce an output array including these 7 different emotion scores. We can visualize each prediction as bar chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyQL0kDKwyxm"
      },
      "outputs": [],
      "source": [
        "def emotion_analysis(emotions):\n",
        "  objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "  y_pos = np.arange(len(objects))\n",
        "  plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
        "  plt.xticks(y_pos, objects)\n",
        "  plt.ylabel('percentage')\n",
        "  plt.title('emotion')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNk8Y4m5b3tf"
      },
      "source": [
        "#**11-Detecting Real-Time Emotion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgAXg8iCb88r"
      },
      "outputs": [],
      "source": [
        "import os  \n",
        "import cv2  \n",
        "import numpy as np  \n",
        "from keras.models import model_from_json  \n",
        "from keras.preprocessing import image  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJbt4FopcEyh"
      },
      "outputs": [],
      "source": [
        "#load model  \n",
        "model = model_from_json(open(\"fer.json\", \"r\").read())  \n",
        "#load weights  \n",
        "model.load_weights('fer.h5') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb9gAA3fcMGx"
      },
      "outputs": [],
      "source": [
        "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srFH4QN7cNW8"
      },
      "outputs": [],
      "source": [
        "cap=cv2.VideoCapture(0)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxitWGrYcUIS"
      },
      "outputs": [],
      "source": [
        "cap=cv2.VideoCapture(0)  \n",
        "while True:  \n",
        "    ret,test_img=cap.read()# captures frame and returns boolean value and captured image  \n",
        "    #if not ret:  \n",
        "        #continue  \n",
        "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)  \n",
        "\n",
        "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)  \n",
        "\n",
        "\n",
        "    for (x,y,w,h) in faces_detected:  \n",
        "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)  \n",
        "        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image  \n",
        "        roi_gray=cv2.resize(roi_gray,(48,48))  \n",
        "        img_pixels = image.img_to_array(roi_gray)  \n",
        "        img_pixels = np.expand_dims(img_pixels, axis = 0)  \n",
        "        img_pixels /= 255  \n",
        "\n",
        "        predictions = model.predict(img_pixels)  \n",
        "\n",
        "        #find max indexed array  \n",
        "        max_index = np.argmax(predictions[0])  \n",
        "\n",
        "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')  \n",
        "        predicted_emotion = emotions[max_index]  \n",
        "\n",
        "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)  \n",
        "\n",
        "    resized_img = cv2.resize(test_img, (1000, 700))  \n",
        "    cv2.imshow('Facial emotion analysis ',resized_img)  \n",
        "\n",
        "\n",
        "\n",
        "    if cv2.waitKey(10) == ord('q'):#wait until 'q' key is pressed  \n",
        "        break  \n",
        "\n",
        "cap.release()  \n",
        "cv2.destroyAllWindows  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm7uTbEWCAei"
      },
      "source": [
        "References:\n",
        "\n",
        "[1] Building a Real Time Emotion Detection with Python\n",
        "\n",
        "https://morioh.com/p/801c509dda99?f=5c21f93bc16e2556b555ab2f\n",
        "\n",
        "[2] Facial Expression Recognition with Keras\n",
        "http://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/\n",
        "\n",
        "[3] Machine Learning Project | Facial Emotion Detection | part 2 Creating Webapp using Flask\n",
        "\n",
        "https://www.youtube.com/watch?v=2mN7ygkc2XU&feature=youtu.be&fbclid=IwAR35sBAYwEakprFrmi12-4wW_54COtb8hcXdEdlCJjQ_en2JCi4zRA28bSs\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Building a Real Time Emotion Detection with Python.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
